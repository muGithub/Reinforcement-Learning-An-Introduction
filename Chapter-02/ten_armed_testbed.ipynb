{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ten_armed_testbed.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muGithub/Reinforcement-Learning-An-Introduction/blob/master/Chapter-02/ten_armed_testbed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3vBBZTRcvtyW",
        "colab_type": "code",
        "outputId": "d10c3b14-8ec4-4c79-fd0d-17ce07113804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
        "# 2016 Artem Oboturov(oboturov@gmail.com)                             #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Bandit:\n",
        "    # @k_arm: # of arms\n",
        "    # @epsilon: probability for exploration in epsilon-greedy algorithm\n",
        "    # @initial: initial estimation for each action\n",
        "    # @step_size: constant step size for updating estimations\n",
        "    # @sample_averages: if True, use sample averages to update estimations instead of constant step size\n",
        "    # @UCB_param: if not None, use UCB algorithm to select action\n",
        "    # @gradient: if True, use gradient based bandit algorithm\n",
        "    # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
        "    def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None,\n",
        "                 gradient=False, gradient_baseline=False, true_reward=0.):\n",
        "        self.k = k_arm\n",
        "        self.step_size = step_size\n",
        "        self.sample_averages = sample_averages\n",
        "        self.indices = np.arange(self.k)\n",
        "        self.time = 0\n",
        "        self.UCB_param = UCB_param\n",
        "        self.gradient = gradient\n",
        "        self.gradient_baseline = gradient_baseline\n",
        "        self.average_reward = 0\n",
        "        self.true_reward = true_reward\n",
        "        self.epsilon = epsilon\n",
        "        self.initial = initial\n",
        "\n",
        "    def reset(self):\n",
        "        # real reward for each action\n",
        "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
        "\n",
        "        # estimation for each action\n",
        "        self.q_estimation = np.zeros(self.k) + self.initial\n",
        "\n",
        "        # # of chosen times for each action\n",
        "        self.action_count = np.zeros(self.k)\n",
        "\n",
        "        self.best_action = np.argmax(self.q_true) \n",
        "\n",
        "    # get an action for this bandit\n",
        "    def act(self):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.indices)\n",
        "\n",
        "        if self.UCB_param is not None:\n",
        "            UCB_estimation = self.q_estimation + \\\n",
        "                     self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5))\n",
        "            q_best = np.max(UCB_estimation)\n",
        "            return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best])\n",
        "\n",
        "        if self.gradient:\n",
        "            exp_est = np.exp(self.q_estimation)\n",
        "            self.action_prob = exp_est / np.sum(exp_est)\n",
        "            return np.random.choice(self.indices, p=self.action_prob)\n",
        "\n",
        "        q_best = np.max(self.q_estimation)\n",
        "        return np.random.choice([action for action, q in enumerate(self.q_estimation) if q == q_best])\n",
        "\n",
        "    # take an action, update estimation for this action\n",
        "    def step(self, action):\n",
        "        # generate the reward under N(real reward, 1)\n",
        "        reward = np.random.randn() + self.q_true[action]\n",
        "        self.time += 1\n",
        "        self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time\n",
        "        self.action_count[action] += 1\n",
        "\n",
        "        if self.sample_averages:\n",
        "            # update estimation using sample averages\n",
        "            self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])\n",
        "        elif self.gradient:\n",
        "            one_hot = np.zeros(self.k)\n",
        "            one_hot[action] = 1\n",
        "            if self.gradient_baseline:\n",
        "                baseline = self.average_reward\n",
        "            else:\n",
        "                baseline = 0\n",
        "            self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob)\n",
        "        else:\n",
        "            # update estimation with constant step size\n",
        "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action])\n",
        "        return reward\n",
        "\n",
        "def simulate(runs, time, bandits):\n",
        "    best_action_counts = np.zeros((len(bandits), runs, time))\n",
        "    rewards = np.zeros(best_action_counts.shape)\n",
        "    for i, bandit in enumerate(bandits):\n",
        "        for r in tqdm(range(runs)):\n",
        "            bandit.reset()\n",
        "            for t in range(time):\n",
        "                action = bandit.act()\n",
        "                reward = bandit.step(action)\n",
        "                rewards[i, r, t] = reward\n",
        "                if action == bandit.best_action:\n",
        "                    best_action_counts[i, r, t] = 1\n",
        "    best_action_counts = best_action_counts.mean(axis=1)\n",
        "    rewards = rewards.mean(axis=1)\n",
        "    return best_action_counts, rewards\n",
        "\n",
        "def figure_2_1():\n",
        "    plt.violinplot(dataset=np.random.randn(200,10) + np.random.randn(10))\n",
        "    plt.xlabel(\"Action\")\n",
        "    plt.ylabel(\"Reward distribution\")\n",
        "    plt.savefig('/content/figure_2_1.png')\n",
        "    plt.close()\n",
        "\n",
        "def figure_2_2(runs=2000, time=1000):\n",
        "    epsilons = [0, 0.1, 0.01]\n",
        "    bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons]\n",
        "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
        "\n",
        "    plt.figure(figsize=(10, 20))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for eps, rewards in zip(epsilons, rewards):\n",
        "        plt.plot(rewards, label='epsilon = %.02f' % (eps))\n",
        "    plt.xlabel('steps')\n",
        "    plt.ylabel('average reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for eps, counts in zip(epsilons, best_action_counts):\n",
        "        plt.plot(counts, label='epsilon = %.02f' % (eps))\n",
        "    plt.xlabel('steps')\n",
        "    plt.ylabel('% optimal action')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/figure_2_2.png')\n",
        "    plt.close()\n",
        "\n",
        "def figure_2_3(runs=2000, time=1000):\n",
        "    bandits = []\n",
        "    bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1))\n",
        "    bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1))\n",
        "    best_action_counts, _ = simulate(runs, time, bandits)\n",
        "\n",
        "    plt.plot(best_action_counts[0], label='epsilon = 0, q = 5')\n",
        "    plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('% optimal action')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/figure_2_3.png')\n",
        "    plt.close()\n",
        "\n",
        "def figure_2_4(runs=2000, time=1000):\n",
        "    bandits = []\n",
        "    bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True))\n",
        "    bandits.append(Bandit(epsilon=0.1, sample_averages=True))\n",
        "    _, average_rewards = simulate(runs, time, bandits)\n",
        "\n",
        "    plt.plot(average_rewards[0], label='UCB c = 2')\n",
        "    plt.plot(average_rewards[1], label='epsilon greedy epsilon = 0.1')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Average reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/figure_2_4.png')\n",
        "    plt.close()\n",
        "\n",
        "def figure_2_5(runs=2000, time=1000):\n",
        "    bandits = []\n",
        "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4))\n",
        "    bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4))\n",
        "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4))\n",
        "    bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4))\n",
        "    best_action_counts, _ = simulate(runs, time, bandits)\n",
        "    labels = ['alpha = 0.1, with baseline',\n",
        "              'alpha = 0.1, without baseline',\n",
        "              'alpha = 0.4, with baseline',\n",
        "              'alpha = 0.4, without baseline']\n",
        "\n",
        "    for i in range(0, len(bandits)):\n",
        "        plt.plot(best_action_counts[i], label=labels[i])\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('% Optimal action')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/figure_2_5.png')\n",
        "    plt.close()\n",
        "\n",
        "def figure_2_6(runs=2000, time=1000):\n",
        "    labels = ['epsilon-greedy', 'gradient bandit',\n",
        "              'UCB', 'optimistic initialization']\n",
        "    generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True),\n",
        "                  lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True),\n",
        "                  lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True),\n",
        "                  lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)]\n",
        "    parameters = [np.arange(-7, -1, dtype=np.float),\n",
        "                  np.arange(-5, 2, dtype=np.float),\n",
        "                  np.arange(-4, 3, dtype=np.float),\n",
        "                  np.arange(-2, 3, dtype=np.float)]\n",
        "\n",
        "    bandits = []\n",
        "    for generator, parameter in zip(generators, parameters):\n",
        "        for param in parameter:\n",
        "            bandits.append(generator(pow(2, param)))\n",
        "\n",
        "    _, average_rewards = simulate(runs, time, bandits)\n",
        "    rewards = np.mean(average_rewards, axis=1)\n",
        "\n",
        "    i = 0\n",
        "    for label, parameter in zip(labels, parameters):\n",
        "        l = len(parameter)\n",
        "        plt.plot(parameter, rewards[i:i+l], label=label)\n",
        "        i += l\n",
        "    plt.xlabel('Parameter(2^x)')\n",
        "    plt.ylabel('Average reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/figure_2_6.png')\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    figure_2_1()\n",
        "    figure_2_2()\n",
        "    figure_2_3()\n",
        "    figure_2_4()\n",
        "    figure_2_5()\n",
        "    figure_2_6()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:37<00:00, 53.05it/s]\n",
            "100%|██████████| 2000/2000 [00:35<00:00, 55.92it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 52.98it/s]\n",
            "100%|██████████| 2000/2000 [00:38<00:00, 51.91it/s]\n",
            "100%|██████████| 2000/2000 [00:35<00:00, 56.05it/s]\n",
            "100%|██████████| 2000/2000 [01:09<00:00, 28.89it/s]\n",
            "100%|██████████| 2000/2000 [00:36<00:00, 55.09it/s]\n",
            "100%|██████████| 2000/2000 [01:32<00:00, 21.37it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.50it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.93it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 20.99it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 52.72it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 52.75it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 53.53it/s]\n",
            "100%|██████████| 2000/2000 [00:36<00:00, 54.19it/s]\n",
            "100%|██████████| 2000/2000 [00:36<00:00, 54.11it/s]\n",
            "100%|██████████| 2000/2000 [00:33<00:00, 60.50it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.49it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.38it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.61it/s]\n",
            "100%|██████████| 2000/2000 [01:33<00:00, 21.75it/s]\n",
            "100%|██████████| 2000/2000 [01:32<00:00, 21.42it/s]\n",
            "100%|██████████| 2000/2000 [01:31<00:00, 22.13it/s]\n",
            "100%|██████████| 2000/2000 [01:32<00:00, 21.54it/s]\n",
            "100%|██████████| 2000/2000 [01:05<00:00, 30.84it/s]\n",
            "100%|██████████| 2000/2000 [01:06<00:00, 30.27it/s]\n",
            "100%|██████████| 2000/2000 [01:06<00:00, 30.25it/s]\n",
            "100%|██████████| 2000/2000 [01:07<00:00, 29.58it/s]\n",
            "100%|██████████| 2000/2000 [01:06<00:00, 29.52it/s]\n",
            "100%|██████████| 2000/2000 [01:09<00:00, 28.74it/s]\n",
            "100%|██████████| 2000/2000 [01:09<00:00, 28.88it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 52.74it/s]\n",
            "100%|██████████| 2000/2000 [00:38<00:00, 52.39it/s]\n",
            "100%|██████████| 2000/2000 [00:38<00:00, 51.49it/s]\n",
            "100%|██████████| 2000/2000 [00:38<00:00, 52.61it/s]\n",
            "100%|██████████| 2000/2000 [00:37<00:00, 53.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lyURuuwz292j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "figure_2_1()\n",
        "figure_2_2()\n",
        "figure_2_3()\n",
        "figure_2_4()\n",
        "figure_2_5()\n",
        "figure_2_6()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}